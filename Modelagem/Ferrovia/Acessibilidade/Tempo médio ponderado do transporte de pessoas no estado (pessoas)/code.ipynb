{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ad07764",
   "metadata": {},
   "source": [
    "# üöÇ Sistema Ollama + FastAPI\n",
    "## Modelos de IA para An√°lise de Transporte Ferrovi√°rio\n",
    "\n",
    "Este notebook permite usar modelos de linguagem local para an√°lises de transporte:\n",
    "- **TinyLLama**: Modelo r√°pido e leve (1.1B par√¢metros)\n",
    "- **Qwen2:1.5b**: Modelo mais preciso (1.5B par√¢metros) - pode ser lento\n",
    "\n",
    "### ‚ö° Como usar:\n",
    "1. **IMPORTANTE**: Certifique-se de que o Docker esteja rodando: `docker compose up -d`\n",
    "2. Execute a **C√©lula 1**: Configura√ß√£o inicial\n",
    "3. Execute a **C√©lula 2**: Verificar modelos dispon√≠veis  \n",
    "4. Execute a **C√©lula 3**: Teste r√°pido\n",
    "5. Use as **C√©lulas 4-5**: Para consultas espec√≠ficas\n",
    "\n",
    "### ‚ö†Ô∏è Notas importantes:\n",
    "- Os modelos podem demorar 30-60s para responder\n",
    "- Se der timeout, tente novamente ou use o TinyLLama\n",
    "- Primeiro download dos modelos pode demorar v√°rios minutos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2aff14bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Cliente configurado\n",
      "‚úÖ URL da API: http://localhost:8000\n",
      "‚úÖ API funcionando: {'status': 'saudavel', 'ollama': 'disponivel', 'url': 'http://ollama-server:11434'}\n"
     ]
    }
   ],
   "source": [
    "# 1. CONFIGURA√á√ÉO INICIAL\n",
    "from chat_client import ChatClient\n",
    "import time\n",
    "\n",
    "# Configurar cliente\n",
    "client = ChatClient(base_url=\"http://localhost:8000\")\n",
    "\n",
    "print(\"‚úÖ Cliente configurado\")\n",
    "print(\"‚úÖ URL da API:\", client.base_url)\n",
    "\n",
    "# Verificar status da API\n",
    "try:\n",
    "    health = client.health_check()\n",
    "    print(\"‚úÖ API funcionando:\", health)\n",
    "except:\n",
    "    print(\"‚ùå API n√£o est√° respondendo. Execute: docker compose up -d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad2dbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. VERIFICAR MODELOS DISPON√çVEIS\n",
    "print(\"Verificando modelos dispon√≠veis...\")\n",
    "\n",
    "# Listar modelos j√° instalados\n",
    "modelos_disponiveis = client.listar_modelos()\n",
    "print(f\"Modelos instalados: {modelos_disponiveis}\")\n",
    "\n",
    "# Verificar se temos os modelos necess√°rios\n",
    "for modelo in modelos_a_baixar:\n",
    "    if modelo in modelos_disponiveis:\n",
    "        print(f\"‚úì {modelo}: dispon√≠vel\")\n",
    "    else:\n",
    "        print(f\"‚úó {modelo}: n√£o encontrado, baixando\")\n",
    "        client.baixar_modelo(modelo)\n",
    "\n",
    "print(\"\\nPronto para usar os modelos dispon√≠veis!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cb8dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. TESTE SIMPLES\n",
    "pergunta = \"O que √© transporte ferrovi√°rio?\"\n",
    "\n",
    "print(\"Testando TinyLLama...\")\n",
    "print(f\"Pergunta: {pergunta}\")\n",
    "print(\"Aguarde... (pode demorar 30-60s)\")\n",
    "\n",
    "try:\n",
    "    inicio = time.time()\n",
    "    resposta = client.chat(pergunta, modelo=\"tinyllama:latest\", stream=False)\n",
    "    tempo = time.time() - inicio\n",
    "    \n",
    "    print(f\"\\nTempo de resposta: {tempo:.1f}s\")\n",
    "    print(f\"\\nResposta do TinyLLama:\")\n",
    "    print(\"=\" * 50)\n",
    "    if 'resposta' in resposta:\n",
    "        print(resposta['resposta'])\n",
    "    elif 'erro' in resposta:\n",
    "        print(f\"Erro: {resposta['erro']}\")\n",
    "    else:\n",
    "        print(str(resposta))\n",
    "    print(\"=\" * 50)\n",
    "    print(\"\\nTeste conclu√≠do! Modelo funcionando.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Erro na conex√£o: {str(e)}\")\n",
    "    print(\"Dica: Verifique se o Docker est√° rodando com 'docker compose up -d'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93741e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. EXEMPLO PRATICO - Analise de Viabilidade\n",
    "modelo_escolhido = \"tinyllama:latest\"  # Modelo mais confiavel\n",
    "\n",
    "pergunta_tecnica = \"\"\"\n",
    "Liste os principais fatores para analisar a viabilidade de uma ferrovia \n",
    "para passageiros em Minas Gerais. Considere aspectos economicos e tecnicos.\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Modelo: {modelo_escolhido}\")\n",
    "print(f\"Pergunta: {pergunta_tecnica.strip()}\")\n",
    "print(\"\\nProcessando... (30-60s)\")\n",
    "\n",
    "try:\n",
    "    inicio = time.time()\n",
    "    resposta = client.chat(pergunta_tecnica, modelo=modelo_escolhido, stream=False)\n",
    "    tempo = time.time() - inicio\n",
    "\n",
    "    print(f\"\\nTempo: {tempo:.1f}s\")\n",
    "    print(f\"\\nAnalise Tecnica:\")\n",
    "    print(\"=\" * 60)\n",
    "    if 'resposta' in resposta:\n",
    "        print(resposta['resposta'])\n",
    "    elif 'erro' in resposta:\n",
    "        print(f\"Erro: {resposta['erro']}\")\n",
    "        print(\"\\nDica: O modelo pode estar ocupado, tente novamente.\")\n",
    "    else:\n",
    "        print(str(resposta))\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Erro: {str(e)}\")\n",
    "    print(\"Dica: Verifique a conexao Docker\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3c228c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. SUA CONSULTA PERSONALIZADA\n",
    "# Modifique a pergunta abaixo e execute para suas proprias analises\n",
    "\n",
    "# Escolha o modelo (tinyllama:latest √© mais rapido e confiavel)\n",
    "modelo = \"tinyllama:latest\"\n",
    "modelo = \"tinydolphin:latest\"  \n",
    "modelo = \"qwen2:1.5b\"  # Modelo mais confiavel\n",
    "# modelo = \"deepcoder:1.5b\"     # Erro de resposta, timeout\n",
    "# modelo = \"exaone-deep:2.4b\"   # Erro de resposta, timeout\n",
    "# modelo = \"stablelm2:1.6b\"     # Resultado 100% errado\n",
    "modelo = \"qwen3:1.7b\"         # Erro de resposta, timeout\n",
    "\n",
    "# Sua pergunta personalizada - MODIFIQUE AQUI:\n",
    "# os dados sempres estar√£o assim:  <key>: <value>\n",
    "\n",
    "print(f\"Modelo selecionado: {modelo}\")\n",
    "print(\"\\nProcessando...\")\n",
    "\n",
    "# Adicionar contexto para melhorar resposta\n",
    "prompt = f\"\"\"\n",
    "voc√™ foi designado a responder o formulario <json> com base nos dados no <contexto>,\n",
    "ent√£o responda rapidamente e com precis√£o.\n",
    "\n",
    "# Formulario JSON:\n",
    "<json>\n",
    "{{\n",
    "    \"Proposta\": value1,\n",
    "    \"C√≥digo\": value2,\n",
    "    \"Categoria\": value3,\n",
    "    \"Tipo de empreendimento\": [\"empreendimento1\", \"empreendimento2\", \"empreendimento3\" ...], # RESPONDA COM OS NOMES DOS EMPREENDIMENTOS\n",
    "\n",
    "    \"Extens√£o (km)\": value5,\n",
    "    \"Tipo bitola\": value6,\n",
    "    \"Total de esta√ß√µes\": value7,\n",
    "    \"Esta√ß√µes atendidas\": [\"esta√ß√£o1\", \"esta√ß√£o2\", \"esta√ß√£o3\" ...], # RESPONDA COM OS NOMES DE ESTA√á√ïES\n",
    "\n",
    "    \"Tempo de viagem ida (min)\": value9,\n",
    "    \"Tempo de viagem ida & volta (min)\": value10,\n",
    "    \"Viagens (m√™s)\": value11,\n",
    "    \"Dias de opera√ß√£o (m√™s)\": value12,\n",
    "    \"Demanda (m√™s)\": value13,\n",
    "    \"Produ√ß√£o quilom√©trica (km/m√™s)\": value14,\n",
    "    \"Tarifa do servi√ßo\": value15,\n",
    "\n",
    "    \"Receita anual (R$)\": value16,\n",
    "    \"Pass.ano/km\": value17,\n",
    "    \"Receita.ano/km\": value18\n",
    "}}\n",
    "</json>\n",
    "\n",
    "# Fonte de dados:\n",
    "<contexto> \n",
    "{contexto}\n",
    "</contexto> \n",
    "\n",
    "<tarefa>\n",
    "extraia os dados da fonde dados com a tag <contexto> e preencha o json com tag <json> nos campos value1, value2, etc.\n",
    "</tarefa>\n",
    "\n",
    "<resposta>\n",
    "retorne o formulario JSON preenchido, e quando falar dele inicie com a tag <json> e quando finalizar insira a tag </json>.\n",
    "</resposta>\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    inicio = time.time()\n",
    "    resposta = client.chat(prompt, modelo=modelo, stream=False, timeout=6000)\n",
    "    tempo = time.time() - inicio\n",
    "\n",
    "    print(f\"\\nTempo: {tempo:.1f}s\")\n",
    "    print(f\"\\nResposta Especializada:\")\n",
    "    print(\"=\" * 60)\n",
    "    if 'resposta' in resposta:\n",
    "        print(resposta['resposta'])\n",
    "    elif 'erro' in resposta:\n",
    "        print(f\"Erro: {resposta['erro']}\")\n",
    "    else:\n",
    "        print(str(resposta))\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(\"\\nPara fazer outra pergunta:\")\n",
    "    print(\"1. Modifique a variavel 'sua_pergunta' acima\")\n",
    "    print(\"2. Execute esta celula novamente\")\n",
    "except Exception as e:\n",
    "    print(f\"Erro: {str(e)}\")\n",
    "    print(\"Verifique se o Docker est√° rodando: docker compose up -d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5ffc78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7890440",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdf_processor import PDFReader, DataStructureDetector\n",
    "import json\n",
    "\n",
    "# Adicionar contexto para melhorar resposta\n",
    "prompt      = \"\"\"\n",
    "voc√™ foi designado a responder o formulario <json> com base nos dados no <contexto>,\n",
    "ent√£o responda com precis√£o, e para ajuda leia as <regras> abaixo:\n",
    "\n",
    "# Regras:\n",
    "<regras>\n",
    "1. Responda com precis√£o e clareza.\n",
    "2. Use os nomes exatos dos empreendimentos e esta√ß√µes conforme aparecem no texto.\n",
    "3. Geralmente o dado esta logo depois dos dois pontos (:).\n",
    "4. Atente-se a casos com mais de uma resposta poss√≠vel, como em \"Tipo de empreendimento\" e \"Esta√ß√µes atendidas\".\n",
    "5. A key do <json> est√° exatamente conforme o formulario, isso facilita a sua busca nos dados em <contexto>\n",
    "6. Se n√£o encontrar um dado, deixe o campo vazio ou com valor nulo.\n",
    "7. Leia os comentarios ## com aten√ß√£o, eles ajudam a entender o que deve ser preenchido.\n",
    "8. Leia a mudan√ßa de topico com #, dentro do contexto tamb√©m e dividido por t√≥picos.\n",
    "9. Ao responder remova os comentarios ## e os divisores de t√≥picos #\n",
    "</regras>\n",
    "\n",
    "# Formulario JSON:\n",
    "<json>\n",
    "{{\n",
    "    \"Proposta\": value1, ## NOME DA PROPOSTA QUE E DIFERENTE DO \"C√≥digo\"\n",
    "    \"C√≥digo\": value2, ## C√ìDIGO DA PROPOSTA QUE E DIFERENTE DA \"Proposta\"\n",
    "    \"Categoria\": value3,\n",
    "    \"Tipo de empreendimento\": [\"empreendimento1\", \"empreendimento2\", \"empreendimento3\" ...], ## RESPONDA COM OS NOMES DOS EMPREENDIMENTOS\n",
    "\n",
    "    # Dados operacionais: \n",
    "    \"Extens√£o (km)\": value5,\n",
    "    \"Tipo bitola\": value6,\n",
    "    \"Total de esta√ß√µes\": value7,\n",
    "    \"Esta√ß√µes atendidas\": [\"esta√ß√£o1\", \"esta√ß√£o2\", \"esta√ß√£o3\" ...], ## RESPONDA COM OS NOMES DE ESTA√á√ïES\n",
    "\n",
    "    # Demanda e receita:\n",
    "    \"Tempo de viagem ida (min)\": value9,\n",
    "    \"Tempo de viagem ida & volta (min)\": value10,\n",
    "    \"Viagens (m√™s)\": value11,\n",
    "    \"Dias de opera√ß√£o (m√™s)\": value12,\n",
    "    \"Demanda (m√™s)\": value13,\n",
    "    \"Produ√ß√£o quilom√©trica (km/m√™s)\": value14,\n",
    "    \"Tarifa do servi√ßo\": value15,\n",
    "\n",
    "    # Desempenho da linha:\n",
    "    \"Receita anual (R$)\": value16,\n",
    "    \"Pass.ano/km\": value17,\n",
    "    \"Receita.ano/km\": value18,\n",
    "    \n",
    "    # Caracter√≠sticas da frota:\n",
    "    \"Frota Total (operacional + reserva)\": value19,\n",
    "    \"Total de carros de passageiros/trem\": value20,\n",
    "    \"Tipo carros\": [\"carro_tipo1\", \"carro_tipo2\", \"carro_tipo3\" ...], ## RESPONDA COM NOME DOS TIPOS DE CARROS\n",
    "    \"Quantidade/composi√ß√£o\": [value21, value22, value23], ## RESPONDA COM OS N√öMERO PARA CADA TIPO CARRO\n",
    "    \"Especifica√ß√£o\": [\"especifica√ß√£o1\", \"especifica√ß√£o2\", \"especifica√ß√£o3\" ...], ## RESPONDA COM AS ESPECIFICA√á√ïES DOS TIPOS DE CARROS LISTADOS\n",
    "    \"Capacidade (PAX/viagem): value21\n",
    "}}\n",
    "</json>\n",
    "\n",
    "# Fonte de dados:\n",
    "<contexto> \n",
    "{}\n",
    "</contexto> \n",
    "\n",
    "<tarefa>\n",
    "extraia os dados da fonde de dados com a tag <contexto> e preencha o formulario com tag <json> nos campos value1, value2, etc.\n",
    "</tarefa>\n",
    "\n",
    "<resposta>\n",
    "retorne o formulario <json> preenchido, ao falar dele abra com <json> e ao terminar de falar dele feche com a tag </json>.\n",
    "</resposta>\n",
    "\"\"\"\n",
    "\n",
    "modelo      = \"qwen3:1.7b\"\n",
    "\n",
    "path_pdf    = \"D:\\\\CODEMGE\\\\PROJETOS\\\\modelagem-transporte-pessoas\\\\Documents\\\\Plano Estrat√©gico Ferrovi√°rio (PEF)\\\\Relatorio_PEF_Minas_2021_ANEXOS.pdf\"\n",
    "PDFTool     = PDFReader(path_pdf, verbose=False)\n",
    "\n",
    "# results     = {}\n",
    "# for page in range(207+2, 244+2, 2):\n",
    "for page in [223, 241]:\n",
    "    if page not in results:\n",
    "        results[page]   = {}\n",
    "    text_start_page     = PDFTool.extract_text_from_page(page)\n",
    "    text_next_page      = PDFTool.extract_text_from_page(page+1)\n",
    "    if text_start_page and text_next_page:\n",
    "        text_start_page = text_start_page[0]\n",
    "        text_next_page  = text_next_page[0]\n",
    "        if \"text\" in text_start_page and \"text\" in text_next_page:\n",
    "            results[page][\"text\"] = text_start_page[\"text\"] + \" \\n\\n \" + text_next_page[\"text\"]\n",
    "            resposta    = client.chat(prompt.format(results[page][\"text\"]), modelo=modelo, stream=False, timeout=6000)\n",
    "            results[page][\"parsed_llm\"] = resposta[\"resposta\"]\n",
    "            with open(\"PARSED_LLM.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "                json.dump(results, file, ensure_ascii=False)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413fa493",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
