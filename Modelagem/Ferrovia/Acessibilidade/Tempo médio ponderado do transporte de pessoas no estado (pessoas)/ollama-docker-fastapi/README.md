# Ollama + FastAPI - Sistema Simplificado

Sistema completo para rodar modelos LLM localmente com Ollama e FastAPI como proxy transparente.

## ğŸ¯ Funcionalidades

- **Ollama**: Servidor LLM rodando em container
- **FastAPI Proxy**: API que atua como proxy transparente para o Ollama
- **Prompts Externos**: Controle total dos prompts fora da aplicaÃ§Ã£o
- **Conversa Direta**: ComunicaÃ§Ã£o direta com os modelos via API

## ğŸš€ InÃ­cio RÃ¡pido

### PrÃ©-requisitos
- Docker e Docker Compose
- Python 3.8+ (para o cliente)

### Uma Ãºnica linha para subir tudo:
```bash
docker compose up -d
```

### Com script automÃ¡tico:
```powershell
# Windows PowerShell
.\start.ps1
```

## ğŸ“ Estrutura Simplificada

```
ollama-docker-fastapi/
â”œâ”€â”€ Dockerfile              # Container da API
â”œâ”€â”€ compose.yml             # OrquestraÃ§Ã£o dos serviÃ§os
â”œâ”€â”€ app.py                  # FastAPI Proxy (transparente)
â”œâ”€â”€ requirements.txt        # DependÃªncias Python
â”œâ”€â”€ chat_client.py          # Cliente Python para teste
â”œâ”€â”€ exemplo_prompts_externos.py  # Exemplos de uso
â”œâ”€â”€ start.ps1              # Script de inicializaÃ§Ã£o
â””â”€â”€ README.md              # Este arquivo
```

## ğŸŒ Endpoints

### FastAPI Proxy (Porta 8000)
- `GET /health` - Status do sistema
- `GET /api/tags` - Listar modelos disponÃ­veis
- `POST /api/generate` - Gerar texto
- `POST /api/chat` - Conversa com contexto
- `POST /api/pull` - Baixar novos modelos
- `GET /docs` - DocumentaÃ§Ã£o automÃ¡tica

### Ollama Direto (Porta 11434)
- Acesso direto ao Ollama (opcional)

## ğŸ’» Uso via Cliente Python

```python
from chat_client import ChatClient

# Inicializar cliente
client = ChatClient()

# Verificar status
status = client.verificar_status()
print(status)

# Listar modelos
modelos = client.listar_modelos()
print(modelos)

# Conversar
resposta = client.conversar_simples(
    "Explique o que Ã© transporte ferroviÃ¡rio",
    modelo="tinyllama"
)
print(resposta)
```

## ğŸ¨ Prompts Externos - Filosofia

O sistema foi projetado para **nÃ£o ter lÃ³gica de prompts internos**. Toda a inteligÃªncia de prompt fica **fora** da aplicaÃ§Ã£o:

```python
# Seus prompts personalizados
prompt_extracao = f"""
VocÃª Ã© um especialista em anÃ¡lise de dados de transporte.
Extraia TODOS os dados do texto: {texto}
Retorne em formato JSON estruturado.
"""

# Envio via proxy (sem modificaÃ§Ã£o)
resultado = client.conversar(
    prompt=prompt_extracao,
    modelo="tinyllama",
    temperature=0.1
)
```

### Vantagens:
- âœ… **Controle total** sobre prompts
- âœ… **Flexibilidade mÃ¡xima** para diferentes casos de uso
- âœ… **FastAPI sÃ³ transita** dados sem modificar
- âœ… **Facilidade de manutenÃ§Ã£o** e teste
- âœ… **ReutilizaÃ§Ã£o** de prompts em diferentes contextos

## ğŸ”§ InstalaÃ§Ã£o de Modelos

```bash
# Via container direto (recomendado)
docker exec ollama-server ollama pull tinyllama

# Verificar modelos instalados
curl http://localhost:8000/api/tags
```

## ğŸ—ï¸ Arquitetura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Seu Cliente   â”‚â”€â”€â”€â–¶â”‚  FastAPI Proxy  â”‚â”€â”€â”€â–¶â”‚     Ollama      â”‚
â”‚  (Prompts)      â”‚    â”‚  (Transparente) â”‚    â”‚   (Modelos)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â–²                           â”‚                       â”‚
      â”‚                           â”‚                       â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    Resposta sem modificaÃ§Ã£o
```

### Fluxo de Dados:
1. **Cliente** envia prompt personalizado
2. **FastAPI Proxy** repassa exatamente como recebido
3. **Ollama** processa com o modelo escolhido
4. **Resposta** retorna sem modificaÃ§Ãµes
5. **Cliente** recebe resposta pura do modelo

## â“ ResoluÃ§Ã£o de Problemas

### Container nÃ£o inicia
```bash
docker compose logs
docker compose down && docker compose up -d
```

### Modelo nÃ£o responde
```bash
# Verificar se modelo existe
curl http://localhost:8000/api/tags

# Baixar modelo se necessÃ¡rio
docker exec ollama-server ollama pull tinyllama
```

### Para parar tudo
```bash
docker compose down
```

## ğŸ¯ Casos de Uso

### 1. ExtraÃ§Ã£o de Dados
```python
prompt = f"Extraia dados tÃ©cnicos do texto: {texto_ferroviario}"
dados = client.conversar(prompt, "tinyllama", temperature=0.1)
```

### 2. AnÃ¡lise de Viabilidade
```python
prompt = f"Analise viabilidade financeira: {dados_projeto}"
analise = client.conversar(prompt, "tinyllama", temperature=0.3)
```

### 3. GeraÃ§Ã£o de RelatÃ³rios
```python
prompt = f"Gere relatÃ³rio executivo: {dados_completos}"
relatorio = client.conversar(prompt, "tinyllama", temperature=0.2)
```
