# Ollama + FastAPI - Sistema Completo

Sistema para rodar modelos LLM localmente com Ollama e FastAPI como proxy transparente.

## üöÄ In√≠cio R√°pido

```bash
# Subir tudo com uma linha
docker compose up -d

# Verificar status
curl http://localhost:8000/health

# Instalar modelo b√°sico
docker exec ollama-server ollama pull tinyllama
```

## üìÅ Arquivos do Projeto

- `compose.yml` - Configura√ß√£o Docker Compose
- `Dockerfile` - Container da API FastAPI
- `app.py` - FastAPI proxy transparente
- `requirements.txt` - Depend√™ncias Python
- `chat_client.py` - Cliente Python para testes
- `exemplo_prompts_externos.py` - Exemplos de uso com prompts externos
- `start.ps1` - Script de inicializa√ß√£o autom√°tica

## üåê Endpoints

- **FastAPI Proxy**: http://localhost:8000
  - `GET /health` - Status do sistema
  - `GET /api/tags` - Listar modelos
  - `POST /api/generate` - Gerar texto
  - `POST /api/chat` - Conversa
  - `GET /docs` - Documenta√ß√£o

- **Ollama Direto**: http://localhost:11434

## üíª Uso

### Via Container (Recomendado)
```bash
# Testar com curl
curl -X POST http://localhost:8000/api/generate \
  -H "Content-Type: application/json" \
  -d '{"model":"tinyllama","prompt":"Explique IA em 2 frases"}'
```

### Via Cliente Python (dentro do container)
```bash
# Executar cliente dentro do container FastAPI
docker exec -it fastapi-proxy python chat_client.py
```

## üéØ Filosofia: Prompts Externos

- **FastAPI apenas transita** dados sem modificar
- **Voc√™ controla 100%** dos prompts
- **Flexibilidade m√°xima** para diferentes casos de uso
- **Sem l√≥gica interna** de prompts

```python
# Exemplo: prompt totalmente seu
meu_prompt = """
Voc√™ √© um especialista em transporte ferrovi√°rio.
Analise este projeto: {dados}
Retorne an√°lise t√©cnica e financeira em JSON.
"""

# Sistema apenas repassa seu prompt
resultado = requests.post("http://localhost:8000/api/generate", 
                         json={"model": "tinyllama", "prompt": meu_prompt})
```

## üîß Comandos √öteis

```bash
# Parar tudo
docker compose down

# Ver logs
docker compose logs -f

# Instalar modelos
docker exec ollama-server ollama pull llama3.2
docker exec ollama-server ollama pull mistral

# Listar modelos instalados
curl http://localhost:8000/api/tags
```

## üìä Performance

- **Startup**: ~30-60 segundos
- **Primeira resposta**: ~10-30 segundos (carrega modelo)
- **Respostas seguintes**: ~2-10 segundos
- **Mem√≥ria**: ~4-8GB (depende do modelo)
