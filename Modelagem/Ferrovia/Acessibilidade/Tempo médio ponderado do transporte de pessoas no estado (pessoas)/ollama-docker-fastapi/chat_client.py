"""
ğŸ¯ Cliente de Chat - Sistema Ollama + FastAPI
Para usar: docker exec -it fastapi-proxy python chat_client.py
"""

import requests
import json
import time


class ChatClient:
    def __init__(self, base_url="http://localhost:8000"):
        self.base_url = base_url
        self.session = requests.Session()
    
    def verificar_status(self):
        """Verifica status do sistema"""
        try:
            response = self.session.get(f"{self.base_url}/health", timeout=5)
            return response.json()
        except Exception as e:
            return {"status": "erro", "erro": str(e)}
    
    def listar_modelos(self):
        """Lista modelos disponÃ­veis"""
        try:
            response = self.session.get(f"{self.base_url}/api/tags", timeout=10)
            if response.status_code == 200:
                data = response.json()
                return [model["name"] for model in data.get("models", [])]
            return []
        except Exception as e:
            print(f"âŒ Erro ao listar modelos: {e}")
            return []
    
    def conversar(self, prompt, modelo="tinyllama", temperature=0.7):
        """Conversa com o modelo"""
        try:
            payload = {
                "model": modelo,
                "prompt": prompt,
                "stream": False,
                "options": {
                    "temperature": temperature
                }
            }
            
            start_time = time.time()
            response = self.session.post(
                f"{self.base_url}/api/generate",
                json=payload,
                timeout=120
            )
            
            if response.status_code == 200:
                result = response.json()
                elapsed = time.time() - start_time
                
                return {
                    "sucesso": True,
                    "resposta": result.get("response", ""),
                    "tempo": round(elapsed, 2),
                    "tokens": result.get("eval_count", 0)
                }
            else:
                return {
                    "sucesso": False,
                    "erro": f"Status {response.status_code}: {response.text}"
                }
                
        except Exception as e:
            return {"sucesso": False, "erro": str(e)}
    
    def conversar_simples(self, prompt, modelo="tinyllama"):
        """Conversa simples - retorna apenas o texto"""
        resultado = self.conversar(prompt, modelo)
        if resultado["sucesso"]:
            return resultado["resposta"]
        else:
            return f"Erro: {resultado['erro']}"


def demonstracao():
    """DemonstraÃ§Ã£o do sistema"""
    print("ğŸ’¬ DEMONSTRAÃ‡ÃƒO - Sistema Ollama + FastAPI")
    print("=" * 60)
    
    client = ChatClient()
    
    # Verificar status
    print("ğŸ“Š Verificando status...")
    status = client.verificar_status()
    print(f"Status: {status}")
    
    if status.get("status") != "saudavel":
        print("âŒ Sistema nÃ£o estÃ¡ saudÃ¡vel. Verifique os containers.")
        return
    
    # Listar modelos
    print("\nğŸ“‹ Modelos disponÃ­veis:")
    modelos = client.listar_modelos()
    if not modelos:
        print("âŒ Nenhum modelo encontrado.")
        print("ğŸ’¡ Execute: docker exec ollama-server ollama pull tinyllama")
        return
    
    for modelo in modelos:
        print(f"  â€¢ {modelo}")
    
    modelo_escolhido = modelos[0]
    print(f"\nğŸ’¬ Testando conversa com: {modelo_escolhido}")
    
    # Testes
    testes = [
        {
            "nome": "Teste Simples",
            "prompt": "Oi! Em uma frase, me diga o que vocÃª faz.",
            "temperatura": 0.3
        },
        {
            "nome": "AnÃ¡lise TÃ©cnica",
            "prompt": """Analise este projeto ferroviÃ¡rio:
            
Projeto: Linha Metro Express
- ExtensÃ£o: 45 km
- EstaÃ§Ãµes: 12
- Velocidade: 80 km/h
- Investimento: R$ 2,1 bilhÃµes
- Capacidade: 60.000 passageiros/dia
- Tarifa: R$ 6,50

Retorne uma anÃ¡lise tÃ©cnica em 3 pontos principais.""",
            "temperatura": 0.2
        }
    ]
    
    for i, teste in enumerate(testes, 1):
        print(f"\nğŸ§ª TESTE {i}: {teste['nome']}")
        print(f"ğŸ“ Prompt: {teste['prompt'][:50]}...")
        
        resultado = client.conversar(
            prompt=teste["prompt"],
            modelo=modelo_escolhido,
            temperature=teste["temperatura"]
        )
        
        if resultado["sucesso"]:
            print(f"âœ… Resposta ({resultado['tempo']}s):")
            print(f"ğŸ“ {resultado['resposta'][:200]}...")
            if resultado["tokens"] > 0:
                print(f"ğŸ”¢ Tokens: {resultado['tokens']}")
        else:
            print(f"âŒ Erro: {resultado['erro']}")
    
    print(f"\nğŸ¨ EXEMPLO: Prompt Personalizado")
    print("=" * 40)
    
    # Exemplo de prompt personalizado para transporte
    prompt_personalizado = """
VocÃª Ã© um consultor especialista em transporte pÃºblico.

CENÃRIO:
Uma cidade de 500.000 habitantes quer implementar um sistema de VLT (VeÃ­culo Leve sobre Trilhos).

DADOS:
- OrÃ§amento disponÃ­vel: R$ 800 milhÃµes
- Demanda estimada: 35.000 passageiros/dia
- ExtensÃ£o proposta: 25 km
- NÃºmero de estaÃ§Ãµes: 18

TAREFA:
Avalie a viabilidade deste projeto em exatamente 4 pontos:
1. Viabilidade Financeira
2. Viabilidade TÃ©cnica  
3. Impacto Social
4. RecomendaÃ§Ã£o Final

Seja direto e tÃ©cnico.
"""
    
    print("ğŸ¤– Executando anÃ¡lise especializada...")
    resultado = client.conversar(
        prompt=prompt_personalizado,
        modelo=modelo_escolhido,
        temperature=0.1  # Baixa criatividade para anÃ¡lise tÃ©cnica
    )
    
    if resultado["sucesso"]:
        print("âœ… AnÃ¡lise de Viabilidade:")
        print(resultado["resposta"])
        print(f"\nâ±ï¸ Tempo: {resultado['tempo']}s")
    else:
        print(f"âŒ Erro: {resultado['erro']}")


if __name__ == "__main__":
    demonstracao()
